{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2262ae28-6e7c-4085-89fd-6da94283f564",
   "metadata": {},
   "source": [
    "# Llama 3.2 1B 모델 파인튜닝 \n",
    "- 타겟 모델 : `llama-3-2-1b 모델`\n",
    "- 참조 github : [참조](https://github.com/aws/amazon-sagemaker-examples/blob/default/%20%20%20%20generative_ai/sm-jumpstart_foundation_llama_3_2_3b_finetuning.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7930dc-2c51-47b9-ad72-053e14685268",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-3-2-1b\", \"1.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c490617-1518-4aea-9de1-546ea8315b17",
   "metadata": {},
   "source": [
    "## 샘플 데이터셋 준비\n",
    "허깅페이스에 공개된 dolly 데이터셋을 다운로드 한다. dolly 데이터셋의 전체는 수십 Gb를 넘는 대량이므로 15k로 만들어 놓은 subset 데이터를 사용해 본다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32616df0-8385-4bff-8889-4097ddd98a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_file = \"train.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ce8f886-bfb7-448e-acf7-ce9be5778d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8a6a2cc1084aa2883792d1df595cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['instruction', 'context', 'response'])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 데이터셋 로드 \n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# summarization 카테고리 데이터만 추출한다.\n",
    "summarization_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == \"summarization\")\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")\n",
    "\n",
    "# 훈련/테스트 데이터셋으로 분리 한다. \n",
    "train_and_test_dataset = summarization_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# jsonl 파일로 변경, 저장한다. jsonl은 한 행이 하나의 row 이다. \"json\"과 달리 행을 변경할 때 \",\"가 없다. \n",
    "train_and_test_dataset[\"train\"].to_json(local_data_file)\n",
    "\n",
    "# 데이터셋이 가지고 있는 column을 살펴 본다. \n",
    "print(train_and_test_dataset[\"train\"][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c23397-2eea-4719-9e24-4ae97e388e6a",
   "metadata": {},
   "source": [
    "## 데이터 템플릿 \n",
    "- 결국 Model에 입력해주는 형식은 `prompt`와 `complete` 형식으로 질문과 정답을 넣어준다. (SFT 학습)\n",
    "- 이중, llama는 `prompt`의 특정 현태를 따르도록 강제하고 있으므로 그 형식에 맞게 가공해 주어야 한다.\n",
    "- 따라서 템플릿은 `prompt` 문자열을 다음과 같은 형태로 만들어 주기 위함이다. \n",
    "```josn\n",
    "{\n",
    "    \"prompt\":\"시스템 메세지\n",
    "    \n",
    "    ### Instruction:\n",
    "    {인스트럭션 문자열}\n",
    "    \n",
    "    ###Input:\n",
    "    {질문 문자열}\n",
    "    \",\n",
    "    \"completeion\":\"{대답 문자열}\"\n",
    "}\n",
    "```\n",
    "\n",
    "이러한 형태를 만들기 위해서 별도의 템플릿 파일 `template.json`을 동일 폴더에 업로드하고, 폴더(버킷)위치를 입력으로 주면 `template.json`을 자동으로 검색하여 적용한다. \n",
    "이때, `train.jsonl`의 컬럼이름과 template의 포맷변수를 정확하게 매칭 시켜 주어야 한다. 가령, `train.jsonl`의 컬럼이름이 `instruction`, `context`, `response`라면, 템플릿에서도 동일한 명칭으로 포맷팅 되어 있어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cd34c40-0469-45c4-8186-c491ff380df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e53afd-0399-4588-9c1e-aaef44608228",
   "metadata": {},
   "source": [
    "## 데이터 버킷 업로드\n",
    "- 모델을 훈련할 때 사용할 데이터를 업로드 한다.\n",
    "- Sagemaker Training Job은 결국 Docker/Container 이므로, Container가 실행되는 환경에 데이터를 삽입해 주어야 한다.\n",
    "- Container를 시작할 때 데이터를 직접 주입해주는 방법도 있으나, 시간/리소스/표준형태여부 등으로 잘 사용하지 않고,\n",
    "- 일반적으로 S3 버켓에 업로드하고 해당 버킷주소를 파라미터로 넘겨주는 방식을 사용한다. fit(`{\"training\": <<s3버킷주소>> }`)\n",
    "- Sagemaker는 컨테이너를 실행 할 때 버킷에서 데이터를 다운로드하여 data 폴더에 복사해 둔다.\n",
    "\n",
    "\n",
    "> [참조] 이러한 데이터를 관련하는 사전정의된 폴더들을 별도 관리한다.\n",
    "- /opt/ml/input/data/{channel_name}: 사용자 정의 채널\n",
    "- /opt/ml/output: 훈련 출력 저장 위치\n",
    "\n",
    "> 이번 예제의 경우, `/opt/ml/input/data/training/` 에 데이터를 복사해 두고, 내부적으로 훈련데이터를 접근할 때 해당 폴더를 참조하게 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece38e2d-683a-4354-82a0-e7d5fdc2bc3e",
   "metadata": {},
   "source": [
    "### [참고] 훈련 환경 (Training Environment)\n",
    "\n",
    "1. 입력 데이터 관련:\n",
    "- /opt/ml/input/data/training: 기본 훈련 데이터 채널\n",
    "- /opt/ml/input/data/validation: 검증 데이터 채널 (사용 시)\n",
    "- /opt/ml/input/data/test: 테스트 데이터 채널 (사용 시)\n",
    "- /opt/ml/input/data/{channel_name}: 사용자 정의 채널\n",
    "\n",
    "2. 설정 관련:\n",
    "- /opt/ml/input/config/hyperparameters.json: 하이퍼파라미터 설정\n",
    "- /opt/ml/input/config/resourceconfig.json: 리소스 구성 정보\n",
    "- /opt/ml/input/config/trainingjobconfig.json: 훈련 작업 구성 정보\n",
    "\n",
    "3. 출력 관련:\n",
    "- /opt/ml/model: 훈련된 모델 저장 위치 (S3에 업로드됨)\n",
    "- /opt/ml/output: 훈련 출력 저장 위치\n",
    "- /opt/ml/output/data: 추가 출력 데이터 저장 위치\n",
    "- /opt/ml/output/failure: 실패 정보 저장 위치\n",
    "\n",
    "4. 체크포인트 관련:\n",
    "- /opt/ml/checkpoints: 모델 체크포인트 저장 위치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c5b2af7-b865-4ab5-b8ba-54a45678b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"my-model-train\"\n",
    "bucket_prefix = \"finetune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1ca107e-b0d3-43b8-baf5-48cfeacb3fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: s3://my-model-train/finetune/dolly_dataset\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "\n",
    "train_data_location = f\"s3://{bucket_name}/{bucket_prefix}/dolly_dataset\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60275d7b-fe63-43d1-a02c-c30f44cb5f53",
   "metadata": {},
   "source": [
    "# 학습 (파인튜닝)\n",
    "Sagemaker / JumpStartEstimator를 이용한 학습 실행\n",
    "- model_id와 버전을 입력합니다.\n",
    "- `environment={\"accept_eula\": \"true\"}`로 EULA 라이선스를 동의 해 주어야 합니다.\n",
    "\n",
    "estimator 생성이 끝나면, `set_hyperparameters`를 통하여 모델 파라미터 세팅해 주고, `fit()`함수를 실행 해 줍니다. \n",
    "- fit()함수를 실행 할 때, `channel_name`으로 `training`에 위에 업로드한 S3버킷주소(폴더)를 입력해 줍니다. (train.jsonl 뿐만아니라 template.json이 포함되어 있으므로 폴더 단위로 입력 해야 합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ab024c8-b392-4080-972a-d428d976a2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using model 'meta-textgeneration-llama-3-2-1b' with wildcard version identifier '1.*'. You can pin to version '1.2.1' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n",
      "INFO:sagemaker:Creating training-job with name: meta-textgeneration-llama-3-2-1b-2025-04-13-09-33-32-320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-13 09:33:33 Starting - Starting the training job\n",
      "2025-04-13 09:33:33 Pending - Training job waiting for capacity...\n",
      "2025-04-13 09:34:03 Pending - Preparing the instances for training...\n",
      "2025-04-13 09:34:30 Downloading - Downloading input data...............\n",
      "2025-04-13 09:36:46 Downloading - Downloading the training image............\n",
      "2025-04-13 09:38:42 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2025-04-13 09:39:06,247 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2025-04-13 09:39:06,265 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-04-13 09:39:06,274 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2025-04-13 09:39:06,276 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-04-13 09:39:15,214 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.33.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/docstring-parser/docstring_parser-0.16-py3-none-any.whl (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/huggingface-hub/huggingface_hub-0.24.2-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cublas-cu12/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-cupti-cu12/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cudnn-cu12/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cufft-cu12/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-curand-cu12/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cusolver-cu12/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cusparse-cu12/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nccl-cu12/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nvjitlink-cu12/nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nvtx-cu12/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 29))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 30))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 31))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 32))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 33))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/shtab/shtab-1.7.1-py3-none-any.whl (from -r requirements.txt (line 34))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 35))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 36))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 37))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 38))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torch/torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (from -r requirements.txt (line 39))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.43.1-py3-none-any.whl (from -r requirements.txt (line 40))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/triton/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 41))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/trl/trl-0.8.1-py3-none-any.whl (from -r requirements.txt (line 42))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/typing-extensions/typing_extensions-4.8.0-py3-none-any.whl (from -r requirements.txt (line 43))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tyro/tyro-0.7.3-py3-none-any.whl (from -r requirements.txt (line 44))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 45))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.2.7-py2.py3-none-any.whl (from -r requirements.txt (line 46))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.14.1->-r requirements.txt (line 5)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 7)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.24.2->-r requirements.txt (line 8)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.43.1->-r requirements.txt (line 40)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro==0.7.3->-r requirements.txt (line 44)) (13.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (2.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0->-r requirements.txt (line 39)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0->-r requirements.txt (line 39)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (0.1.0)\u001b[0m\n",
      "\u001b[34mscipy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=4fb7e04acd11971eb56ba8661be3ade47bf93448f1e5c197e88bec9ab7283960\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[34mSuccessfully built fire\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, Brotli, bitsandbytes, typing-extensions, triton, tokenize-rt, termcolor, shtab, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, safetensors, pyzstd, pyppmd, pycryptodomex, pybcj, pathspec, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, multivolumefile, loralib, inflate64, docstring-parser, py7zr, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, fire, black, tyro, tokenizers, nvidia-cusolver-cu12, transformers, torch, datasets, accelerate, trl, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing_extensions 4.7.1\u001b[0m\n",
      "\u001b[34mUninstalling typing_extensions-4.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing_extensions-4.7.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: triton\u001b[0m\n",
      "\u001b[34mFound existing installation: triton 2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mUninstalling triton-2.0.0.dev20221202:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled triton-2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.20.3\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.20.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.20.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.16.1\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.16.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.16.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 accelerate-0.33.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 docstring-parser-0.16 fire-0.5.0 huggingface-hub-0.24.2 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pyzstd-0.15.9 safetensors-0.4.2 sagemaker-jumpstart-huggingface-script-utilities-1.2.7 sagemaker-jumpstart-script-utilities-1.1.9 shtab-1.7.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 tokenizers-0.19.1 torch-2.2.0 transformers-4.43.1 triton-2.2.0 trl-0.8.1 typing-extensions-4.8.0 tyro-0.7.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2025-04-13 09:40:33,799 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-04-13 09:40:33,799 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-04-13 09:40:33,838 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-04-13 09:40:33,866 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-04-13 09:40:33,893 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-04-13 09:40:33,902 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"chat_template\": \"Llama3.1\",\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"1\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"target_modules\": \"q_proj,v_proj\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-3-2-1b-2025-04-13-09-33-32-320\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"chat_template\":\"Llama3.1\",\"enable_fsdp\":\"True\",\"epoch\":\"1\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"target_modules\":\"q_proj,v_proj\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"chat_template\":\"Llama3.1\",\"enable_fsdp\":\"True\",\"epoch\":\"1\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"target_modules\":\"q_proj,v_proj\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-3-2-1b-2025-04-13-09-33-32-320\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--chat_dataset\",\"False\",\"--chat_template\",\"Llama3.1\",\"--enable_fsdp\",\"True\",\"--epoch\",\"1\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--target_modules\",\"q_proj,v_proj\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_DATASET=False\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_TEMPLATE=Llama3.1\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=1\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[34mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TARGET_MODULES=q_proj,v_proj\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --chat_dataset False --chat_template Llama3.1 --enable_fsdp True --epoch 1 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --target_modules q_proj,v_proj --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34m2025-04-13 09:40:33,941 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '1', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '1', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '1', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--target_modules', 'q_proj,v_proj', '--chat_template', 'Llama3.1', '--enable_fsdp', '--add_input_output_demarcation_key', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[34m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 14979.66it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 733.78it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1069 examples [00:00, 117144.64 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 17770.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 3571.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 3390.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/1069 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 1000/1069 [00:00<00:00, 2159.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 1069/1069 [00:00<00:00, 2125.50 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34m--> /opt/ml/additonals3data has 1235.8144 Million params\u001b[0m\n",
      "\u001b[34mtrainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.06889230774326355\u001b[0m\n",
      "\u001b[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mINFO:root:--> Training Set Length = 367\u001b[0m\n",
      "\u001b[34mINFO:root:--> Validation Set Length = 92\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/91 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.3+cuda12.3\u001b[0m\n",
      "\u001b[34malgo-1:57:91 [0] nccl_net_ofi_init:1444 NCCL WARN NET/OFI Only EFA provider is supported\u001b[0m\n",
      "\u001b[34malgo-1:57:91 [0] nccl_net_ofi_init:1483 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 2.1555137634277344\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 1/91 [00:01<02:16,  1.52s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 1.965362310409546\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 2/91 [00:02<01:36,  1.09s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 1.8631168603897095\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 3/91 [00:03<01:23,  1.05it/s]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 1.9291410446166992\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 4/91 [00:03<01:17,  1.12it/s]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 2.1346871852874756\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 5/91 [00:04<01:13,  1.17it/s]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 1.9077770709991455\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 6/91 [00:05<01:10,  1.20it/s]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 1.8376293182373047\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 7/91 [00:06<01:08,  1.22it/s]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 1.8368613719940186\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 8/91 [00:07<01:07,  1.24it/s]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 2.201188087463379\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 9/91 [00:07<01:05,  1.24it/s]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 2.0009984970092773\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 10/91 [00:08<01:04,  1.25it/s]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 2.052875280380249\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 11/91 [00:09<01:03,  1.25it/s]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 2.163156032562256\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 12/91 [00:10<01:02,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 2.0684521198272705\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 13/91 [00:11<01:01,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 2.281627655029297\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 14/91 [00:11<01:01,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 1.925902247428894\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 15/91 [00:12<01:00,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 1.9589980840682983\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 16/91 [00:13<00:59,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 1.7281039953231812\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 17/91 [00:14<00:58,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 2.2322335243225098\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 18/91 [00:14<00:57,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 2.0918548107147217\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 19/91 [00:15<00:56,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 1.838236689567566\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 20/91 [00:16<00:56,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 2.014626979827881\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 21/91 [00:17<00:55,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 2.220097064971924\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 22/91 [00:18<00:54,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 2.0820701122283936\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 23/91 [00:18<00:53,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 2.0058083534240723\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 24/91 [00:19<00:52,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 1.8778245449066162\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 25/91 [00:20<00:52,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 2.2820963859558105\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 26/91 [00:21<00:51,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 2.4357945919036865\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 27/91 [00:22<00:50,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 1.9933760166168213\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 28/91 [00:22<00:49,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 1.7800301313400269\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 29/91 [00:23<00:49,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 1.85819411277771\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 30/91 [00:24<00:48,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 2.0132951736450195\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 31/91 [00:25<00:47,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 1.9826035499572754\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 32/91 [00:26<00:46,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 2.216944456100464\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 33/91 [00:26<00:45,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 1.8936556577682495\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 34/91 [00:27<00:46,  1.23it/s]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 1.8963911533355713\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 35/91 [00:28<00:45,  1.24it/s]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 2.037576913833618\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 36/91 [00:29<00:44,  1.25it/s]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 2.2634615898132324\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 37/91 [00:30<00:43,  1.25it/s]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 2.088383436203003\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 38/91 [00:30<00:42,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 2.0351297855377197\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 39/91 [00:31<00:41,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 1.8126635551452637\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 40/91 [00:32<00:40,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 2.0928232669830322\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 41/91 [00:33<00:39,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 1.9649397134780884\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 42/91 [00:34<00:38,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 2.0472724437713623\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 43/91 [00:34<00:38,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 2.1501245498657227\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  48%|#033[34m████▊     #033[0m| 44/91 [00:35<00:37,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 1.9108203649520874\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  49%|#033[34m████▉     #033[0m| 45/91 [00:36<00:36,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 1.8608064651489258\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  51%|#033[34m█████     #033[0m| 46/91 [00:37<00:35,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 1.9270929098129272\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  52%|#033[34m█████▏    #033[0m| 47/91 [00:37<00:34,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 2.153272867202759\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  53%|#033[34m█████▎    #033[0m| 48/91 [00:38<00:34,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 1.7782291173934937\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  54%|#033[34m█████▍    #033[0m| 49/91 [00:39<00:33,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 1.8587337732315063\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  55%|#033[34m█████▍    #033[0m| 50/91 [00:40<00:32,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 1.6121246814727783\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  56%|#033[34m█████▌    #033[0m| 51/91 [00:41<00:31,  1.27it/s]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 1.8199421167373657\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  57%|#033[34m█████▋    #033[0m| 52/91 [00:41<00:30,  1.27it/s]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 1.72254478931427\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  58%|#033[34m█████▊    #033[0m| 53/91 [00:42<00:30,  1.27it/s]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 2.0575225353240967\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  59%|#033[34m█████▉    #033[0m| 54/91 [00:43<00:29,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 1.7389681339263916\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  60%|#033[34m██████    #033[0m| 55/91 [00:44<00:28,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 55 is completed and loss is 2.0148239135742188\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  62%|#033[34m██████▏   #033[0m| 56/91 [00:45<00:27,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 56 is completed and loss is 1.9488674402236938\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  63%|#033[34m██████▎   #033[0m| 57/91 [00:45<00:26,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 57 is completed and loss is 2.0856313705444336\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  64%|#033[34m██████▎   #033[0m| 58/91 [00:46<00:26,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 58 is completed and loss is 2.0635759830474854\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  65%|#033[34m██████▍   #033[0m| 59/91 [00:47<00:25,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 59 is completed and loss is 1.8808287382125854\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  66%|#033[34m██████▌   #033[0m| 60/91 [00:48<00:24,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 60 is completed and loss is 1.9949734210968018\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  67%|#033[34m██████▋   #033[0m| 61/91 [00:49<00:23,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 61 is completed and loss is 1.9319097995758057\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  68%|#033[34m██████▊   #033[0m| 62/91 [00:49<00:22,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 62 is completed and loss is 1.6644339561462402\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  69%|#033[34m██████▉   #033[0m| 63/91 [00:50<00:23,  1.22it/s]\u001b[0m\n",
      "\u001b[34mstep 63 is completed and loss is 1.81088125705719\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  70%|#033[34m███████   #033[0m| 64/91 [00:51<00:21,  1.23it/s]\u001b[0m\n",
      "\u001b[34mstep 64 is completed and loss is 1.6634734869003296\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  71%|#033[34m███████▏  #033[0m| 65/91 [00:52<00:20,  1.24it/s]\u001b[0m\n",
      "\u001b[34mstep 65 is completed and loss is 1.7345943450927734\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  73%|#033[34m███████▎  #033[0m| 66/91 [00:53<00:20,  1.25it/s]\u001b[0m\n",
      "\u001b[34mstep 66 is completed and loss is 1.9459142684936523\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  74%|#033[34m███████▎  #033[0m| 67/91 [00:53<00:19,  1.25it/s]\u001b[0m\n",
      "\u001b[34mstep 67 is completed and loss is 2.2695200443267822\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  75%|#033[34m███████▍  #033[0m| 68/91 [00:54<00:18,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 68 is completed and loss is 1.8096964359283447\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  76%|#033[34m███████▌  #033[0m| 69/91 [00:55<00:17,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 69 is completed and loss is 2.155021905899048\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  77%|#033[34m███████▋  #033[0m| 70/91 [00:56<00:16,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 70 is completed and loss is 1.900796890258789\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  78%|#033[34m███████▊  #033[0m| 71/91 [00:57<00:15,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 71 is completed and loss is 1.7077075242996216\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  79%|#033[34m███████▉  #033[0m| 72/91 [00:57<00:15,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 72 is completed and loss is 1.7612863779067993\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  80%|#033[34m████████  #033[0m| 73/91 [00:58<00:14,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 73 is completed and loss is 1.9495404958724976\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  81%|#033[34m████████▏ #033[0m| 74/91 [00:59<00:13,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 74 is completed and loss is 1.6232784986495972\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  82%|#033[34m████████▏ #033[0m| 75/91 [01:00<00:12,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 75 is completed and loss is 1.6387856006622314\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  84%|#033[34m████████▎ #033[0m| 76/91 [01:01<00:11,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 76 is completed and loss is 1.9074416160583496\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  85%|#033[34m████████▍ #033[0m| 77/91 [01:01<00:11,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 77 is completed and loss is 1.9929183721542358\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  86%|#033[34m████████▌ #033[0m| 78/91 [01:02<00:10,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 78 is completed and loss is 1.8994029760360718\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  87%|#033[34m████████▋ #033[0m| 79/91 [01:03<00:09,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 79 is completed and loss is 1.8239666223526\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  88%|#033[34m████████▊ #033[0m| 80/91 [01:04<00:08,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 80 is completed and loss is 1.803261399269104\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  89%|#033[34m████████▉ #033[0m| 81/91 [01:04<00:07,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 81 is completed and loss is 1.7648108005523682\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  90%|#033[34m█████████ #033[0m| 82/91 [01:05<00:07,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 82 is completed and loss is 2.042518377304077\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  91%|#033[34m█████████ #033[0m| 83/91 [01:06<00:06,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 83 is completed and loss is 1.7719961404800415\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  92%|#033[34m█████████▏#033[0m| 84/91 [01:07<00:05,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 84 is completed and loss is 1.8192514181137085\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  93%|#033[34m█████████▎#033[0m| 85/91 [01:08<00:04,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 85 is completed and loss is 1.7062804698944092\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  95%|#033[34m█████████▍#033[0m| 86/91 [01:08<00:03,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 86 is completed and loss is 1.6800191402435303\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  96%|#033[34m█████████▌#033[0m| 87/91 [01:09<00:03,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 87 is completed and loss is 1.7509777545928955\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  97%|#033[34m█████████▋#033[0m| 88/91 [01:10<00:02,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 88 is completed and loss is 1.530480980873108\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  98%|#033[34m█████████▊#033[0m| 89/91 [01:11<00:01,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 89 is completed and loss is 1.9010863304138184\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  99%|#033[34m█████████▉#033[0m| 90/91 [01:12<00:00,  1.26it/s]\u001b[0m\n",
      "\u001b[34mstep 90 is completed and loss is 2.1324479579925537\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 91/91 [01:12<00:00,  1.26it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 91/91 [01:12<00:00,  1.25it/s]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 8 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 9 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 8 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 3 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/92 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   1%|#033[32m          #033[0m| 1/92 [00:00<00:11,  7.82it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   2%|#033[32m▏         #033[0m| 2/92 [00:00<00:10,  8.41it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   3%|#033[32m▎         #033[0m| 3/92 [00:00<00:10,  8.62it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   4%|#033[32m▍         #033[0m| 4/92 [00:00<00:09,  8.85it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   5%|#033[32m▌         #033[0m| 5/92 [00:00<00:09,  8.81it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   7%|#033[32m▋         #033[0m| 6/92 [00:00<00:09,  8.89it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   8%|#033[32m▊         #033[0m| 7/92 [00:00<00:09,  8.97it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   9%|#033[32m▊         #033[0m| 8/92 [00:00<00:09,  9.01it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  10%|#033[32m▉         #033[0m| 9/92 [00:01<00:09,  9.06it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  11%|#033[32m█         #033[0m| 10/92 [00:01<00:09,  9.07it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  12%|#033[32m█▏        #033[0m| 11/92 [00:01<00:08,  9.01it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  13%|#033[32m█▎        #033[0m| 12/92 [00:01<00:09,  8.49it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  14%|#033[32m█▍        #033[0m| 13/92 [00:01<00:09,  8.71it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  15%|#033[32m█▌        #033[0m| 14/92 [00:01<00:08,  8.74it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  16%|#033[32m█▋        #033[0m| 15/92 [00:01<00:08,  8.81it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  17%|#033[32m█▋        #033[0m| 16/92 [00:01<00:08,  8.98it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  18%|#033[32m█▊        #033[0m| 17/92 [00:01<00:08,  9.03it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  20%|#033[32m█▉        #033[0m| 18/92 [00:02<00:08,  9.08it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  21%|#033[32m██        #033[0m| 19/92 [00:02<00:08,  9.10it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  22%|#033[32m██▏       #033[0m| 20/92 [00:02<00:07,  9.09it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  23%|#033[32m██▎       #033[0m| 21/92 [00:02<00:07,  9.04it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  24%|#033[32m██▍       #033[0m| 22/92 [00:02<00:07,  9.06it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  25%|#033[32m██▌       #033[0m| 23/92 [00:02<00:07,  9.02it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  26%|#033[32m██▌       #033[0m| 24/92 [00:02<00:07,  9.09it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  27%|#033[32m██▋       #033[0m| 25/92 [00:02<00:07,  9.11it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  28%|#033[32m██▊       #033[0m| 26/92 [00:02<00:07,  9.18it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  29%|#033[32m██▉       #033[0m| 27/92 [00:03<00:07,  9.19it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  30%|#033[32m███       #033[0m| 28/92 [00:03<00:07,  8.94it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  32%|#033[32m███▏      #033[0m| 29/92 [00:03<00:06,  9.04it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  33%|#033[32m███▎      #033[0m| 30/92 [00:03<00:06,  9.09it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  34%|#033[32m███▎      #033[0m| 31/92 [00:03<00:06,  9.16it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  35%|#033[32m███▍      #033[0m| 32/92 [00:03<00:06,  9.16it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  36%|#033[32m███▌      #033[0m| 33/92 [00:03<00:06,  9.15it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  37%|#033[32m███▋      #033[0m| 34/92 [00:03<00:06,  9.15it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  38%|#033[32m███▊      #033[0m| 35/92 [00:03<00:06,  9.11it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  39%|#033[32m███▉      #033[0m| 36/92 [00:04<00:06,  9.09it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  40%|#033[32m████      #033[0m| 37/92 [00:04<00:06,  8.83it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  41%|#033[32m████▏     #033[0m| 38/92 [00:04<00:06,  8.85it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  42%|#033[32m████▏     #033[0m| 39/92 [00:04<00:05,  8.87it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  43%|#033[32m████▎     #033[0m| 40/92 [00:04<00:05,  8.88it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  45%|#033[32m████▍     #033[0m| 41/92 [00:04<00:05,  8.95it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  46%|#033[32m████▌     #033[0m| 42/92 [00:04<00:05,  8.96it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  47%|#033[32m████▋     #033[0m| 43/92 [00:04<00:05,  9.01it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  48%|#033[32m████▊     #033[0m| 44/92 [00:04<00:05,  9.03it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  49%|#033[32m████▉     #033[0m| 45/92 [00:05<00:05,  9.06it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  50%|#033[32m█████     #033[0m| 46/92 [00:05<00:05,  9.07it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  51%|#033[32m█████     #033[0m| 47/92 [00:05<00:04,  9.08it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  52%|#033[32m█████▏    #033[0m| 48/92 [00:05<00:04,  8.98it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  53%|#033[32m█████▎    #033[0m| 49/92 [00:05<00:04,  8.99it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  54%|#033[32m█████▍    #033[0m| 50/92 [00:05<00:04,  8.95it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  55%|#033[32m█████▌    #033[0m| 51/92 [00:05<00:04,  8.83it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  57%|#033[32m█████▋    #033[0m| 52/92 [00:05<00:04,  8.97it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  58%|#033[32m█████▊    #033[0m| 53/92 [00:05<00:04,  9.01it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  59%|#033[32m█████▊    #033[0m| 54/92 [00:06<00:04,  9.10it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  60%|#033[32m█████▉    #033[0m| 55/92 [00:06<00:04,  9.11it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  61%|#033[32m██████    #033[0m| 56/92 [00:06<00:03,  9.09it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  62%|#033[32m██████▏   #033[0m| 57/92 [00:06<00:03,  9.10it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  63%|#033[32m██████▎   #033[0m| 58/92 [00:06<00:03,  9.15it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  64%|#033[32m██████▍   #033[0m| 59/92 [00:06<00:03,  9.08it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  65%|#033[32m██████▌   #033[0m| 60/92 [00:06<00:03,  9.05it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  66%|#033[32m██████▋   #033[0m| 61/92 [00:06<00:03,  9.08it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  67%|#033[32m██████▋   #033[0m| 62/92 [00:06<00:03,  9.13it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  68%|#033[32m██████▊   #033[0m| 63/92 [00:07<00:03,  9.12it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  70%|#033[32m██████▉   #033[0m| 64/92 [00:07<00:03,  9.10it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  71%|#033[32m███████   #033[0m| 65/92 [00:07<00:02,  9.12it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  72%|#033[32m███████▏  #033[0m| 66/92 [00:07<00:02,  9.12it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  73%|#033[32m███████▎  #033[0m| 67/92 [00:07<00:02,  9.12it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  74%|#033[32m███████▍  #033[0m| 68/92 [00:07<00:02,  9.11it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  75%|#033[32m███████▌  #033[0m| 69/92 [00:07<00:02,  9.12it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  76%|#033[32m███████▌  #033[0m| 70/92 [00:07<00:02,  9.12it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  77%|#033[32m███████▋  #033[0m| 71/92 [00:07<00:02,  9.11it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  78%|#033[32m███████▊  #033[0m| 72/92 [00:07<00:02,  9.18it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  79%|#033[32m███████▉  #033[0m| 73/92 [00:08<00:02,  9.15it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  80%|#033[32m████████  #033[0m| 74/92 [00:08<00:01,  9.11it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  82%|#033[32m████████▏ #033[0m| 75/92 [00:08<00:01,  9.08it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  83%|#033[32m████████▎ #033[0m| 76/92 [00:08<00:01,  9.03it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  84%|#033[32m████████▎ #033[0m| 77/92 [00:08<00:01,  9.05it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  85%|#033[32m████████▍ #033[0m| 78/92 [00:08<00:01,  9.06it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  86%|#033[32m████████▌ #033[0m| 79/92 [00:08<00:01,  9.10it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  87%|#033[32m████████▋ #033[0m| 80/92 [00:08<00:01,  9.09it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  88%|#033[32m████████▊ #033[0m| 81/92 [00:08<00:01,  9.09it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  89%|#033[32m████████▉ #033[0m| 82/92 [00:09<00:01,  9.09it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  90%|#033[32m█████████ #033[0m| 83/92 [00:09<00:00,  9.05it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  91%|#033[32m█████████▏#033[0m| 84/92 [00:09<00:00,  9.04it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  92%|#033[32m█████████▏#033[0m| 85/92 [00:09<00:00,  8.99it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  93%|#033[32m█████████▎#033[0m| 86/92 [00:09<00:00,  9.02it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  95%|#033[32m█████████▍#033[0m| 87/92 [00:09<00:00,  9.03it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  96%|#033[32m█████████▌#033[0m| 88/92 [00:09<00:00,  9.03it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  97%|#033[32m█████████▋#033[0m| 89/92 [00:09<00:00,  9.04it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  98%|#033[32m█████████▊#033[0m| 90/92 [00:09<00:00,  9.09it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:  99%|#033[32m█████████▉#033[0m| 91/92 [00:10<00:00,  9.11it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 92/92 [00:10<00:00,  9.06it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 92/92 [00:10<00:00,  9.02it/s]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(6.8289, device='cuda:0') eval_epoch_loss=tensor(1.9212, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 0 is 1.9211616516113281\u001b[0m\n",
      "\u001b[34mEpoch 1: train_perplexity=6.9742, train_epoch_loss=1.9422, epcoh time 73.12733356399997s\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_prep, Value: 6.974166393280029\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_loss, Value: 1.942212700843811\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_prep, Value: 6.828886985778809\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_loss, Value: 1.9211616516113281\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_epoch_time, Value: 73.12733356399997\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_checkpoint_time, Value: 0.05386885699999766\u001b[0m\n",
      "\u001b[34mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001b[0m\n",
      "\u001b[34mINFO:root:Saving the combined model in safetensors format.\u001b[0m\n",
      "\u001b[34mINFO:root:Saving complete.\u001b[0m\n",
      "\u001b[34mINFO:root:Copying tokenizer to the output directory.\u001b[0m\n",
      "\u001b[34mINFO:root:Putting inference code with the fine-tuned model directory.\u001b[0m\n",
      "\u001b[34m2025-04-13 09:42:26,367 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-04-13 09:42:26,367 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-04-13 09:42:26,367 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2025-04-13 09:42:33 Uploading - Uploading generated training model\n",
      "2025-04-13 09:42:51 Completed - Training job completed\n",
      "Training seconds: 501\n",
      "Billable seconds: 501\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    environment={\"accept_eula\": \"true\"},  # EULA 동의\n",
    "    disable_output_compression=True,\n",
    ")\n",
    "\n",
    "estimator.set_hyperparameters(instruction_tuned=\"True\", epoch=\"1\", max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf19db84-a902-4a3d-951d-5261c07bc5db",
   "metadata": {},
   "source": [
    "# Deployment \n",
    "파인튜닝한 모델의 결과물은, sagemaker container 내부 `/opt/ml/model`에 저장되는데 훈련이 종료되는 시점에 자동으로 S3에 업로드 됩니다. 업로드 S3 위치는 (특별히 지정하지 않는다면) Sagemaker의 기본 버킷하위에 `{job_name}/output/model/`위치에 저장됩니다. \n",
    "- 해당 bucket의 위치를 model 파일로 하여 deploy 할 수 도 있고,\n",
    "- 아래처럼 훈련 제어 변수인 `estimator.deploy()`를 통해서 배포할 수 도 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd0a3e62-0c27-45ca-b205-2c5e244626fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for inference hosting endpoint. Defaulting to ml.g6.xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g6.xlarge.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-3-2-1b-2025-04-13-12-13-02-516\n",
      "INFO:sagemaker:Creating endpoint-config with name 03-llama-basic\n",
      "INFO:sagemaker:Creating endpoint with name 03-llama-basic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!"
     ]
    }
   ],
   "source": [
    "finetuned_predictor = estimator.deploy(endpoint_name=\"03-llama-basic\") # 엔트포인트 이름에 언더바`_` 불가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18fe50d7-cace-468e-bb74-44ec81c0806b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.base_predictor.Predictor at 0x7f97fbd4e310>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5205c507-934a-430a-b75c-13f40036af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template_inference = template['prompt']\n",
    "\n",
    "instruction = \"너의 이름은 무엇이야?\"\n",
    "input_output_marker = \"\\n\\n### Response:\\n\"\n",
    "prompt = template[\"prompt\"].format(instruction=instruction, context=context)\n",
    "prompt += input_output_marker\n",
    "\n",
    "prompt_payload = {\n",
    "    'inputs':prompt,\n",
    "    'parameters': {'max_new_tokens': 512, 'top_p': 0.9,'temperature': 0.6}\n",
    "    # 'parameters': {'max_new_tokens': 512, 'top_p': 0.9,'temperature': 0.6, 'details': True}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa90cefa-8ff5-4a48-9ae4-f13b005ef024",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = finetuned_predictor.predict(prompt_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e6df729-10bb-44de-bfdc-e6daac028ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 이상재는 대한민국의 전직 축구 선수이자 현 축구 감독이다.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefb33ef-cf17-44ce-aa6d-f35a74fb70d7",
   "metadata": {},
   "source": [
    "# Inference with sagemaker runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "04d7895f-135a-44d2-ae05-6fe9e3487ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response:\n",
      " 이상재는 대한민국의 전직 축구 선수이자 현 축구 감독이다.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# SageMaker Runtime 클라이언트 생성\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime', region_name='us-west-2')\n",
    "\n",
    "# 엔드포인트 이름\n",
    "endpoint_name = \"03-llama-basic\"\n",
    "\n",
    "\n",
    "# JSON 형식으로 데이터 직렬화\n",
    "payload = json.dumps(prompt_payload)\n",
    "\n",
    "# 엔드포인트 호출\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,  # 엔드포인트 이름\n",
    "    Body=payload,               # 입력 데이터\n",
    "    ContentType='application/json'  # 데이터 형식\n",
    ")\n",
    "\n",
    "# 응답 처리\n",
    "result = json.loads(response['Body'].read().decode('utf-8'))\n",
    "print(\"Model Response:\\n\", result['generated_text'].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dd1352-e0f2-4c25-8c84-0053f97fc4c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a8511e-686a-44f6-8c95-07a372de45c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
