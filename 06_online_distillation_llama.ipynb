{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd6924a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers datasets accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c050be3-9fb0-48cf-bf96-164e790c5f83",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ ì¦ë¥˜ (ì˜¨ë¼ì¸ ëŸ¬ë‹)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b1188-95b4-45ab-b05e-50502edadcd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f81dcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894ef724-dd84-45d6-a230-8920fa143df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, InitProcessGroupKwargs\n",
    "from accelerate.utils import FullyShardedDataParallelPlugin\n",
    "\n",
    "# Accelerator ì´ˆê¸°í™”\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1884a2e8-4ddc-4307-a78c-167475768b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sharding_strategy is deprecated in favor of reshard_after_forward. This will be removed in a future version of Accelerate.\n"
     ]
    }
   ],
   "source": [
    "# 2. í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "training_args = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 2,  # GPUë‹¹ ë°°ì¹˜ í¬ê¸°\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 3,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# 3. Accelerate ì„¤ì •\n",
    "# FSDP ì„¤ì •\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    sharding_strategy=\"FULL_SHARD\",  # ì™„ì „ ë¶„ì‚° ëª¨ë“œ\n",
    "    cpu_offload=True,  # CPU ì˜¤í”„ë¡œë”© í™œì„±í™”\n",
    "    # mixed_precision=\"bf16\",  # BF16 í˜¼í•© ì •ë°€ë„ ì‚¬ìš©\n",
    "    auto_wrap_policy=\"TRANSFORMER_BASED_WRAP\",  # íŠ¸ëœìŠ¤í¬ë¨¸ ë ˆì´ì–´ ìë™ ë˜í•‘\n",
    "    # transformer_layer_cls_to_wrap=[\"LlamaDecoderLayer\"],  # ë˜í•‘í•  ë ˆì´ì–´ í´ë˜ìŠ¤\n",
    "    backward_prefetch=\"BACKWARD_PRE\",  # ë°±ì›Œë“œ íŒ¨ìŠ¤ ìµœì í™”\n",
    "    activation_checkpointing=True,  # í™œì„±í™” ì²´í¬í¬ì¸íŒ… ì‚¬ìš©\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ffbcde7-411e-4f86-8c22-0287d012a07f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# í”„ë¡œì íŠ¸ êµ¬ì„±\n",
    "OUTPUT_DIR = \"outputs/\"\n",
    "project_config = ProjectConfiguration(\n",
    "    project_dir=OUTPUT_DIR,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    ")\n",
    "\n",
    "# í”„ë¡œì„¸ìŠ¤ ê·¸ë£¹ ì´ˆê¸°í™” ì„¤ì •\n",
    "init_kwargs = InitProcessGroupKwargs()  # 30ë¶„ íƒ€ì„ì•„ì›ƒ\n",
    "\n",
    "# Accelerator ì´ˆê¸°í™”\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=training_args[\"gradient_accumulation_steps\"],\n",
    "    log_with=\"tensorboard\",\n",
    "    project_config=project_config,\n",
    "    fsdp_plugin=fsdp_plugin,\n",
    "    kwargs_handlers=[init_kwargs],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8a35ca",
   "metadata": {},
   "source": [
    "### ğŸ”¤ ì…ë ¥ ë°ì´í„° í¬ë§· ì˜ˆì‹œ (`mini_ai_name_data.jsonl`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12ec26f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONL í˜•ì‹ ì˜ˆì‹œ\n",
    "# {\"instruction\": \"ë„ˆì˜ ì´ë¦„ì´ ë­ì•¼?\", \"output\": \"ì œ ì´ë¦„ì€ ë¯¸ë‹ˆaiì…ë‹ˆë‹¤.\"}\n",
    "# {\"instruction\": \"ìê¸°ì†Œê°œ í•´ë´\", \"output\": \"ì €ëŠ” ë¯¸ë‹ˆaië¼ê³  í•©ë‹ˆë‹¤.\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a092f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282138d957524999945ba5d741a54732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278cbc48d1154925ac5c11eaec36c0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "teacher_model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "student_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(teacher_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "eos_token = tokenizer.eos_token\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"dataset/kogpt/kochatgpt_1_SFT.jsonl\")[\"train\"]\n",
    "\n",
    "colname_1, colname_2 = \"prompt\", \"completion\"\n",
    "\n",
    "\n",
    "def tokenize(example):\n",
    "    prompt = f\"### ì§ˆë¬¸:\\n{example[colname_1]}\\n\\n### ë‹µë³€:\\n\"\n",
    "    target = example[colname_2]\n",
    "    full = prompt + target\n",
    "    tokens = tokenizer(full, truncation=True, max_length=512, padding=\"max_length\")\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=False).remove_columns([colname_1, colname_2])\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e4e0511-1a05-45f7-b4c4-75d2fc2bbbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 12000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33150361-c0c6-476f-8f8b-cf7768d247e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5326d776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 15:54:47.372916: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-14 15:54:47.387615: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-14 15:54:47.391594: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-14 15:54:47.401717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ac62ca16d8476683ec298193c74ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher = AutoModelForCausalLM.from_pretrained(teacher_model_id,  device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "student = AutoModelForCausalLM.from_pretrained(student_model_id,  device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "temperature = 2.0\n",
    "loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "optimizer = AdamW(student.parameters(), lr=2e-5)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "student, teacher, optimizer, dataloader = accelerator.prepare(student, teacher, optimizer, dataloader)\n",
    "teacher.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caa5f649-406c-4578-85bc-9d8df79de810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55f22e09-83d5-4ec5-921b-f43e4bd21cb4",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ í›ˆë ¨ (ì„ ìƒë‹˜ ëª¨ë¸ -> í•™ìƒ ëª¨ë¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85026aa1-5490-4240-8331-7fcaa3f1338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=4, shuffle=True, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a94430a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1933/3000 [26:24<14:34,  1.22it/s, loss=68.5]"
     ]
    }
   ],
   "source": [
    "N_EPOCH = 5\n",
    "\n",
    "for epoch in range(N_EPOCH):\n",
    "    student.train()\n",
    "    loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch in loop:\n",
    "        input_ids = batch[\"input_ids\"].to(student.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(student.device)\n",
    "\n",
    "        # Student forward\n",
    "        student_outputs = student(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        student_logits = student_outputs.logits / temperature\n",
    "\n",
    "        # Teacher forward\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            teacher_logits = teacher_outputs.logits / temperature\n",
    "\n",
    "        student_logits = student_logits[:, :-1, :].contiguous()\n",
    "        teacher_logits = teacher_logits[:, :-1, :].contiguous()\n",
    "\n",
    "        target_probs = torch.nn.functional.softmax(teacher_logits, dim=-1)\n",
    "        student_log_probs = torch.nn.functional.log_softmax(student_logits, dim=-1)\n",
    "\n",
    "        loss = loss_fn(student_log_probs, target_probs)\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2b2cc8-c943-45a0-853d-d9f2b8863a88",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23cb3c97-4c8e-45f3-b08b-c0f81142fd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./outputs/distilled-mini-ai/tokenizer_config.json',\n",
       " './outputs/distilled-mini-ai/special_tokens_map.json',\n",
       " './outputs/distilled-mini-ai/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student.save_pretrained(\"./outputs/distilled-mini-ai\")\n",
    "tokenizer.save_pretrained(\"./outputs/distilled-mini-ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74157d3f-3f46-4a29-b8ae-bac4be555d77",
   "metadata": {},
   "source": [
    "# ëª¨ë¸ í…ŒìŠ¤íŠ¸/ê²°ê³¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbb048bb-f05e-474d-925b-4fcf1c713c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def test(prompt: str, model_path: str = \"./outputs/distilled-mini-ai\", max_new_tokens: int = 100):\n",
    "    \"\"\"\n",
    "    í•™ìŠµëœ student ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸ (ì˜ˆ: \"ë„ˆì˜ ì´ë¦„ì´ ë­ì•¼?\")\n",
    "        model_path (str): íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë””ë ‰í„°ë¦¬ ê²½ë¡œ\n",
    "        max_new_tokens (int): ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜\n",
    "    \"\"\"\n",
    "    # ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "    # íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "    # í”„ë¡¬í”„íŠ¸ ì •ì œ\n",
    "    full_prompt = f\"\"\"### ì§ˆë¬¸:\n",
    "{prompt}\n",
    "\n",
    "### ë‹µë³€:\"\"\"\n",
    "\n",
    "    # ìƒì„±\n",
    "    output = pipe(full_prompt, max_new_tokens=max_new_tokens, do_sample=True, eos_token_id=128009,repetition_penalty=1.2)[0][\"generated_text\"]\n",
    "    \n",
    "    # ì¶œë ¥ ê²°ê³¼\n",
    "    print(\"ì…ë ¥ í”„ë¡¬í”„íŠ¸:\")\n",
    "    print(full_prompt)\n",
    "    print(\"\\n ëª¨ë¸ ì‘ë‹µ:\")\n",
    "    print(output.split(\"### ë‹µë³€:\")[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae24aa32-5bfc-4280-b023-a44f0531b557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ í”„ë¡¬í”„íŠ¸:\n",
      "### ì§ˆë¬¸:\n",
      "ì²œì—°ìœ¼ë¡œ ì—¼ìƒ‰í•˜ê³  ì‹¶ì€ë° ì§€ì†ì´ ê½¤ ë˜ë‚˜ìš”?\n",
      "\n",
      "### ë‹µë³€:\n",
      "\n",
      " ëª¨ë¸ ì‘ë‹µ:\n",
      "ì§€ì†ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì „ê¸°í™”ë¬¼ì´ë‚˜ ìˆ˜ìš©ì„± ë¬¼ì§ˆì„ ì‚¬ìš©í•  ë•Œ, ì „ê¸°í™”ë¬¼ ë˜ëŠ” ìˆ˜ìš©ì„± ë¬¼ì§ˆì— ëŒ€í•œ ì¬ìƒì˜ ì‹œê°„ì´ ì•½ 2-5ë…„ë¡œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê¸°ê°„ì€ ì›ì • ë° ì •ìƒì ì¸ ìš©ëŸ‰ì— ë”°ë¼ ë‹¤ë¥´ìŠµë‹ˆë‹¤.\n",
      "\n",
      "*   ì „ê¸°í™”ë¬¼(ì „ê¸°í™”ë¬¼): 1-3ë…„\n",
      "    *   ê¸°ì¥: 1-2ë…„ (0.01%\n"
     ]
    }
   ],
   "source": [
    "# test(\"ë„ˆì˜ ì´ë¦„ì´ ë­ì•¼?\")\n",
    "test(\"ì²œì—°ìœ¼ë¡œ ì—¼ìƒ‰í•˜ê³  ì‹¶ì€ë° ì§€ì†ì´ ê½¤ ë˜ë‚˜ìš”?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dbd70f-ecab-41da-b499-7b485a9a257b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
