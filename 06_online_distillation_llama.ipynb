{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd6924a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers datasets accelerate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c050be3-9fb0-48cf-bf96-164e790c5f83",
   "metadata": {},
   "source": [
    "# 모델 증류 (온라인 러닝)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b1188-95b4-45ab-b05e-50502edadcd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f81dcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894ef724-dd84-45d6-a230-8920fa143df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, InitProcessGroupKwargs\n",
    "from accelerate.utils import FullyShardedDataParallelPlugin\n",
    "\n",
    "# Accelerator 초기화\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1884a2e8-4ddc-4307-a78c-167475768b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sharding_strategy is deprecated in favor of reshard_after_forward. This will be removed in a future version of Accelerate.\n"
     ]
    }
   ],
   "source": [
    "# 2. 학습 파라미터 설정\n",
    "training_args = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"per_device_train_batch_size\": 2,  # GPU당 배치 크기\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 3,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# 3. Accelerate 설정\n",
    "# FSDP 설정\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    sharding_strategy=\"FULL_SHARD\",  # 완전 분산 모드\n",
    "    cpu_offload=True,  # CPU 오프로딩 활성화\n",
    "    # mixed_precision=\"bf16\",  # BF16 혼합 정밀도 사용\n",
    "    auto_wrap_policy=\"TRANSFORMER_BASED_WRAP\",  # 트랜스포머 레이어 자동 래핑\n",
    "    # transformer_layer_cls_to_wrap=[\"LlamaDecoderLayer\"],  # 래핑할 레이어 클래스\n",
    "    backward_prefetch=\"BACKWARD_PRE\",  # 백워드 패스 최적화\n",
    "    activation_checkpointing=True,  # 활성화 체크포인팅 사용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ffbcde7-411e-4f86-8c22-0287d012a07f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 프로젝트 구성\n",
    "OUTPUT_DIR = \"outputs/\"\n",
    "project_config = ProjectConfiguration(\n",
    "    project_dir=OUTPUT_DIR,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    ")\n",
    "\n",
    "# 프로세스 그룹 초기화 설정\n",
    "init_kwargs = InitProcessGroupKwargs()  # 30분 타임아웃\n",
    "\n",
    "# Accelerator 초기화\n",
    "accelerator = Accelerator(\n",
    "    gradient_accumulation_steps=training_args[\"gradient_accumulation_steps\"],\n",
    "    log_with=\"tensorboard\",\n",
    "    project_config=project_config,\n",
    "    fsdp_plugin=fsdp_plugin,\n",
    "    kwargs_handlers=[init_kwargs],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8a35ca",
   "metadata": {},
   "source": [
    "### 🔤 입력 데이터 포맷 예시 (`mini_ai_name_data.jsonl`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12ec26f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONL 형식 예시\n",
    "# {\"instruction\": \"너의 이름이 뭐야?\", \"output\": \"제 이름은 미니ai입니다.\"}\n",
    "# {\"instruction\": \"자기소개 해봐\", \"output\": \"저는 미니ai라고 합니다.\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a092f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282138d957524999945ba5d741a54732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278cbc48d1154925ac5c11eaec36c0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "teacher_model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "student_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(teacher_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "eos_token = tokenizer.eos_token\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"dataset/kogpt/kochatgpt_1_SFT.jsonl\")[\"train\"]\n",
    "\n",
    "colname_1, colname_2 = \"prompt\", \"completion\"\n",
    "\n",
    "\n",
    "def tokenize(example):\n",
    "    prompt = f\"### 질문:\\n{example[colname_1]}\\n\\n### 답변:\\n\"\n",
    "    target = example[colname_2]\n",
    "    full = prompt + target\n",
    "    tokens = tokenizer(full, truncation=True, max_length=512, padding=\"max_length\")\n",
    "    return tokens\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=False).remove_columns([colname_1, colname_2])\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e4e0511-1a05-45f7-b4c4-75d2fc2bbbf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 12000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33150361-c0c6-476f-8f8b-cf7768d247e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5326d776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-14 15:54:47.372916: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-14 15:54:47.387615: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-14 15:54:47.391594: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-14 15:54:47.401717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ac62ca16d8476683ec298193c74ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher = AutoModelForCausalLM.from_pretrained(teacher_model_id,  device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "student = AutoModelForCausalLM.from_pretrained(student_model_id,  device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "temperature = 2.0\n",
    "loss_fn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "optimizer = AdamW(student.parameters(), lr=2e-5)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "student, teacher, optimizer, dataloader = accelerator.prepare(student, teacher, optimizer, dataloader)\n",
    "teacher.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caa5f649-406c-4578-85bc-9d8df79de810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55f22e09-83d5-4ec5-921b-f43e4bd21cb4",
   "metadata": {},
   "source": [
    "# 모델 훈련 (선생님 모델 -> 학생 모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85026aa1-5490-4240-8331-7fcaa3f1338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=4, shuffle=True, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a94430a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  64%|██████▍   | 1933/3000 [26:24<14:34,  1.22it/s, loss=68.5]"
     ]
    }
   ],
   "source": [
    "N_EPOCH = 5\n",
    "\n",
    "for epoch in range(N_EPOCH):\n",
    "    student.train()\n",
    "    loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch in loop:\n",
    "        input_ids = batch[\"input_ids\"].to(student.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(student.device)\n",
    "\n",
    "        # Student forward\n",
    "        student_outputs = student(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        student_logits = student_outputs.logits / temperature\n",
    "\n",
    "        # Teacher forward\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            teacher_logits = teacher_outputs.logits / temperature\n",
    "\n",
    "        student_logits = student_logits[:, :-1, :].contiguous()\n",
    "        teacher_logits = teacher_logits[:, :-1, :].contiguous()\n",
    "\n",
    "        target_probs = torch.nn.functional.softmax(teacher_logits, dim=-1)\n",
    "        student_log_probs = torch.nn.functional.log_softmax(student_logits, dim=-1)\n",
    "\n",
    "        loss = loss_fn(student_log_probs, target_probs)\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2b2cc8-c943-45a0-853d-d9f2b8863a88",
   "metadata": {},
   "source": [
    "# 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23cb3c97-4c8e-45f3-b08b-c0f81142fd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./outputs/distilled-mini-ai/tokenizer_config.json',\n",
       " './outputs/distilled-mini-ai/special_tokens_map.json',\n",
       " './outputs/distilled-mini-ai/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student.save_pretrained(\"./outputs/distilled-mini-ai\")\n",
    "tokenizer.save_pretrained(\"./outputs/distilled-mini-ai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74157d3f-3f46-4a29-b8ae-bac4be555d77",
   "metadata": {},
   "source": [
    "# 모델 테스트/결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbb048bb-f05e-474d-925b-4fcf1c713c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def test(prompt: str, model_path: str = \"./outputs/distilled-mini-ai\", max_new_tokens: int = 100):\n",
    "    \"\"\"\n",
    "    학습된 student 모델을 테스트하는 함수입니다.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): 질문 프롬프트 (예: \"너의 이름이 뭐야?\")\n",
    "        model_path (str): 파인튜닝된 모델 디렉터리 경로\n",
    "        max_new_tokens (int): 생성할 최대 토큰 수\n",
    "    \"\"\"\n",
    "    # 모델 & 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "    # 파이프라인 생성\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "    # 프롬프트 정제\n",
    "    full_prompt = f\"\"\"### 질문:\n",
    "{prompt}\n",
    "\n",
    "### 답변:\"\"\"\n",
    "\n",
    "    # 생성\n",
    "    output = pipe(full_prompt, max_new_tokens=max_new_tokens, do_sample=True, eos_token_id=128009,repetition_penalty=1.2)[0][\"generated_text\"]\n",
    "    \n",
    "    # 출력 결과\n",
    "    print(\"입력 프롬프트:\")\n",
    "    print(full_prompt)\n",
    "    print(\"\\n 모델 응답:\")\n",
    "    print(output.split(\"### 답변:\")[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae24aa32-5bfc-4280-b023-a44f0531b557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 프롬프트:\n",
      "### 질문:\n",
      "천연으로 염색하고 싶은데 지속이 꽤 되나요?\n",
      "\n",
      "### 답변:\n",
      "\n",
      " 모델 응답:\n",
      "지속적으로 사용하는 전기화물이나 수용성 물질을 사용할 때, 전기화물 또는 수용성 물질에 대한 재생의 시간이 약 2-5년로 할 수 있습니다. 이 기간은 원정 및 정상적인 용량에 따라 다르습니다.\n",
      "\n",
      "*   전기화물(전기화물): 1-3년\n",
      "    *   기장: 1-2년 (0.01%\n"
     ]
    }
   ],
   "source": [
    "# test(\"너의 이름이 뭐야?\")\n",
    "test(\"천연으로 염색하고 싶은데 지속이 꽤 되나요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dbd70f-ecab-41da-b499-7b485a9a257b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
