{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2262ae28-6e7c-4085-89fd-6da94283f564",
   "metadata": {},
   "source": [
    "# Llama 3.2 1B 모델 파인튜닝 \n",
    "- 타겟 모델 : `llama-3-2-1b 모델` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21d6928c-4935-4bb2-8e03-621098bd0747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import json \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d7930dc-2c51-47b9-ad72-053e14685268",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-3-2-1b\", \"1.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c490617-1518-4aea-9de1-546ea8315b17",
   "metadata": {},
   "source": [
    "## 샘플 데이터셋 준비\n",
    "허깅페이스에 공개된 dolly 데이터셋을 다운로드 한다. dolly 데이터셋의 전체는 수십 Gb를 넘는 대량이므로 15k로 만들어 놓은 subset 데이터를 사용해 본다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32616df0-8385-4bff-8889-4097ddd98a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_data_dir = \"./dataset/my_kor\"\n",
    "custom_data_file = \"train.jsonl\" \n",
    "local_data_file = f\"{custom_data_dir}/{custom_data_file}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ce8f886-bfb7-448e-acf7-ce9be5778d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AWS EC2에 대해 설명해주세요</td>\n",
       "      <td>클라우드 컴퓨팅 서비스에 대해 알아보고 있어요.</td>\n",
       "      <td>Amazon EC2(Elastic Compute Cloud)는 AWS에서 제공하는 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amazon S3의 스토리지 클래스에 대해 설명해주세요</td>\n",
       "      <td>비용 효율적인 데이터 저장 방법을 찾고 있습니다.</td>\n",
       "      <td>Amazon S3는 다양한 스토리지 클래스를 제공합니다. S3 Standard는 자...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon SageMaker란 무엇인가요?</td>\n",
       "      <td>머신러닝 모델을 개발하고 배포하는 방법을 알고 싶습니다.</td>\n",
       "      <td>Amazon SageMaker는 AWS에서 제공하는 완전 관리형 머신러닝 서비스입니...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AWS Direct Connect의 주요 이점을 설명해주세요</td>\n",
       "      <td>온프레미스 환경과 AWS를 연결하는 방법을 찾고 있습니다.</td>\n",
       "      <td>AWS Direct Connect는 온프레미스 환경에서 AWS로 전용 네트워크 연결...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon GuardDuty의 주요 기능을 설명해주세요</td>\n",
       "      <td>AWS 환경의 보안 위협을 탐지하는 방법을 찾고 있습니다.</td>\n",
       "      <td>Amazon GuardDuty는 AWS 계정과 워크로드를 보호하기 위한 지능형 위협...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>sLM 모델 파인튜닝 세션에서 다루는 주요 내용은 무엇인가요?</td>\n",
       "      <td>이상재 AI/ML Sr. Specialist Solutions Architect는 ...</td>\n",
       "      <td>sLM 모델 파인튜닝 세션에서는 다음과 같은 주요 내용을 다룰 예정입니다: 1) s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>AWS와 KCC 협력의 목적은 무엇인가요?</td>\n",
       "      <td>AWS와 KCC 간에 긴밀한 협력을 통해 2025년 4월 15일 이상재 AI/ML ...</td>\n",
       "      <td>AWS와 KCC 간의 협력 목적은 AI/ML 기술 역량 강화와 지식 공유에 있습니다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>LLM 파인튜닝의 중요성에 대해 설명해주세요.</td>\n",
       "      <td>이상재 AI/ML Sr. Specialist Solutions Architect는 ...</td>\n",
       "      <td>LLM 파인튜닝의 중요성은 다음과 같은 여러 측면에서 찾아볼 수 있습니다. 첫째, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>sLM과 LLM의 차이점은 무엇인가요?</td>\n",
       "      <td>이상재 AI/ML Sr. Specialist Solutions Architect는 ...</td>\n",
       "      <td>sLM(small Language Model)과 LLM(Large Language ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>파인튜닝을 위한 데이터셋 준비 방법에 대해 설명해주세요.</td>\n",
       "      <td>이상재 AI/ML Sr. Specialist Solutions Architect는 ...</td>\n",
       "      <td>파인튜닝을 위한 데이터셋 준비는 성공적인 모델 학습의 핵심입니다. 첫째, 데이터 수...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           instruction  \\\n",
       "0                   AWS EC2에 대해 설명해주세요   \n",
       "1       Amazon S3의 스토리지 클래스에 대해 설명해주세요   \n",
       "2             Amazon SageMaker란 무엇인가요?   \n",
       "3    AWS Direct Connect의 주요 이점을 설명해주세요   \n",
       "4      Amazon GuardDuty의 주요 기능을 설명해주세요   \n",
       "..                                 ...   \n",
       "59  sLM 모델 파인튜닝 세션에서 다루는 주요 내용은 무엇인가요?   \n",
       "60             AWS와 KCC 협력의 목적은 무엇인가요?   \n",
       "61           LLM 파인튜닝의 중요성에 대해 설명해주세요.   \n",
       "62               sLM과 LLM의 차이점은 무엇인가요?   \n",
       "63     파인튜닝을 위한 데이터셋 준비 방법에 대해 설명해주세요.   \n",
       "\n",
       "                                              context  \\\n",
       "0                          클라우드 컴퓨팅 서비스에 대해 알아보고 있어요.   \n",
       "1                         비용 효율적인 데이터 저장 방법을 찾고 있습니다.   \n",
       "2                     머신러닝 모델을 개발하고 배포하는 방법을 알고 싶습니다.   \n",
       "3                    온프레미스 환경과 AWS를 연결하는 방법을 찾고 있습니다.   \n",
       "4                    AWS 환경의 보안 위협을 탐지하는 방법을 찾고 있습니다.   \n",
       "..                                                ...   \n",
       "59  이상재 AI/ML Sr. Specialist Solutions Architect는 ...   \n",
       "60  AWS와 KCC 간에 긴밀한 협력을 통해 2025년 4월 15일 이상재 AI/ML ...   \n",
       "61  이상재 AI/ML Sr. Specialist Solutions Architect는 ...   \n",
       "62  이상재 AI/ML Sr. Specialist Solutions Architect는 ...   \n",
       "63  이상재 AI/ML Sr. Specialist Solutions Architect는 ...   \n",
       "\n",
       "                                             response  \n",
       "0   Amazon EC2(Elastic Compute Cloud)는 AWS에서 제공하는 ...  \n",
       "1   Amazon S3는 다양한 스토리지 클래스를 제공합니다. S3 Standard는 자...  \n",
       "2   Amazon SageMaker는 AWS에서 제공하는 완전 관리형 머신러닝 서비스입니...  \n",
       "3   AWS Direct Connect는 온프레미스 환경에서 AWS로 전용 네트워크 연결...  \n",
       "4   Amazon GuardDuty는 AWS 계정과 워크로드를 보호하기 위한 지능형 위협...  \n",
       "..                                                ...  \n",
       "59  sLM 모델 파인튜닝 세션에서는 다음과 같은 주요 내용을 다룰 예정입니다: 1) s...  \n",
       "60  AWS와 KCC 간의 협력 목적은 AI/ML 기술 역량 강화와 지식 공유에 있습니다...  \n",
       "61  LLM 파인튜닝의 중요성은 다음과 같은 여러 측면에서 찾아볼 수 있습니다. 첫째, ...  \n",
       "62  sLM(small Language Model)과 LLM(Large Language ...  \n",
       "63  파인튜닝을 위한 데이터셋 준비는 성공적인 모델 학습의 핵심입니다. 첫째, 데이터 수...  \n",
       "\n",
       "[64 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(local_data_file, lines=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c23397-2eea-4719-9e24-4ae97e388e6a",
   "metadata": {},
   "source": [
    "## 데이터 템플릿 \n",
    "- 결국 Model에 입력해주는 형식은 `prompt`와 `complete` 형식으로 질문과 정답을 넣어준다. (SFT 학습)\n",
    "- 이중, llama는 `prompt`의 특정 현태를 따르도록 강제하고 있으므로 그 형식에 맞게 가공해 주어야 한다.\n",
    "- 따라서 템플릿은 `prompt` 문자열을 다음과 같은 형태로 만들어 주기 위함이다. \n",
    "```josn\n",
    "{\n",
    "    \"prompt\":\"시스템 메세지\n",
    "    \n",
    "    ### Instruction:\n",
    "    {인스트럭션 문자열}\n",
    "    \n",
    "    ###Input:\n",
    "    {질문 문자열}\n",
    "    \",\n",
    "    \"completeion\":\"{대답 문자열}\"\n",
    "}\n",
    "```\n",
    "\n",
    "이러한 형태를 만들기 위해서 별도의 템플릿 파일 `template.json`을 동일 폴더에 업로드하고, 폴더(버킷)위치를 입력으로 주면 `template.json`을 자동으로 검색하여 적용한다. \n",
    "이때, `train.jsonl`의 컬럼이름과 template의 포맷변수를 정확하게 매칭 시켜 주어야 한다. 가령, `train.jsonl`의 컬럼이름이 `instruction`, `context`, `response`라면, 템플릿에서도 동일한 명칭으로 포맷팅 되어 있어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cd34c40-0469-45c4-8186-c491ff380df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "with open(f\"{custom_data_dir}/template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e53afd-0399-4588-9c1e-aaef44608228",
   "metadata": {},
   "source": [
    "## 데이터 버킷 업로드\n",
    "- 모델을 훈련할 때 사용할 데이터를 업로드 한다.\n",
    "- Sagemaker Training Job은 결국 Docker/Container 이므로, Container가 실행되는 환경에 데이터를 삽입해 주어야 한다.\n",
    "- Container를 시작할 때 데이터를 직접 주입해주는 방법도 있으나, 시간/리소스/표준형태여부 등으로 잘 사용하지 않고,\n",
    "- 일반적으로 S3 버켓에 업로드하고 해당 버킷주소를 파라미터로 넘겨주는 방식을 사용한다. fit(`{\"training\": <<s3버킷주소>> }`)\n",
    "- Sagemaker는 컨테이너를 실행 할 때 버킷에서 데이터를 다운로드하여 data 폴더에 복사해 둔다.\n",
    "\n",
    "\n",
    "> [참조] 이러한 데이터를 관련하는 사전정의된 폴더들을 별도 관리한다.\n",
    "- /opt/ml/input/data/{channel_name}: 사용자 정의 채널\n",
    "- /opt/ml/output: 훈련 출력 저장 위치\n",
    "\n",
    "> 이번 예제의 경우, `/opt/ml/input/data/training/` 에 데이터를 복사해 두고, 내부적으로 훈련데이터를 접근할 때 해당 폴더를 참조하게 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece38e2d-683a-4354-82a0-e7d5fdc2bc3e",
   "metadata": {},
   "source": [
    "### [참고] 훈련 환경 (Training Environment)\n",
    "\n",
    "1. 입력 데이터 관련:\n",
    "- /opt/ml/input/data/training: 기본 훈련 데이터 채널\n",
    "- /opt/ml/input/data/validation: 검증 데이터 채널 (사용 시)\n",
    "- /opt/ml/input/data/test: 테스트 데이터 채널 (사용 시)\n",
    "- /opt/ml/input/data/{channel_name}: 사용자 정의 채널\n",
    "\n",
    "2. 설정 관련:\n",
    "- /opt/ml/input/config/hyperparameters.json: 하이퍼파라미터 설정\n",
    "- /opt/ml/input/config/resourceconfig.json: 리소스 구성 정보\n",
    "- /opt/ml/input/config/trainingjobconfig.json: 훈련 작업 구성 정보\n",
    "\n",
    "3. 출력 관련:\n",
    "- /opt/ml/model: 훈련된 모델 저장 위치 (S3에 업로드됨)\n",
    "- /opt/ml/output: 훈련 출력 저장 위치\n",
    "- /opt/ml/output/data: 추가 출력 데이터 저장 위치\n",
    "- /opt/ml/output/failure: 실패 정보 저장 위치\n",
    "\n",
    "4. 체크포인트 관련:\n",
    "- /opt/ml/checkpoints: 모델 체크포인트 저장 위치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c5b2af7-b865-4ab5-b8ba-54a45678b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"my-model-train\"\n",
    "bucket_prefix = \"finetune/llama3-2-1b/custom_kor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1ca107e-b0d3-43b8-baf5-48cfeacb3fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: s3://my-model-train/finetune/llama3-2-1b/custom_kor\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "\n",
    "train_data_location = f\"s3://{bucket_name}/{bucket_prefix}\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60275d7b-fe63-43d1-a02c-c30f44cb5f53",
   "metadata": {},
   "source": [
    "# 학습 (파인튜닝)\n",
    "Sagemaker / JumpStartEstimator를 이용한 학습 실행\n",
    "- model_id와 버전을 입력합니다.\n",
    "- `environment={\"accept_eula\": \"true\"}`로 EULA 라이선스를 동의 해 주어야 합니다.\n",
    "\n",
    "estimator 생성이 끝나면, `set_hyperparameters`를 통하여 모델 파라미터 세팅해 주고, `fit()`함수를 실행 해 줍니다. \n",
    "- fit()함수를 실행 할 때, `channel_name`으로 `training`에 위에 업로드한 S3버킷주소(폴더)를 입력해 줍니다. (train.jsonl 뿐만아니라 template.json이 포함되어 있으므로 폴더 단위로 입력 해야 합니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8ab024c8-b392-4080-972a-d428d976a2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: meta-textgeneration-llama-3-2-1b-2025-04-13-14-20-50-328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-13 14:20:51 Starting - Starting the training job\n",
      "2025-04-13 14:20:51 Pending - Training job waiting for capacity......\n",
      "2025-04-13 14:21:40 Pending - Preparing the instances for training...\n",
      "2025-04-13 14:22:07 Downloading - Downloading input data............\n",
      "2025-04-13 14:24:18 Downloading - Downloading the training image............\n",
      "2025-04-13 14:26:19 Training - Training image download completed. Training in progress.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2025-04-13 14:26:50,407 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2025-04-13 14:26:50,444 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-04-13 14:26:50,452 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2025-04-13 14:26:50,454 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-04-13 14:26:59,206 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.33.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/docstring-parser/docstring_parser-0.16-py3-none-any.whl (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/huggingface-hub/huggingface_hub-0.24.2-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cublas-cu12/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-cupti-cu12/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cudnn-cu12/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cufft-cu12/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-curand-cu12/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cusolver-cu12/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cusparse-cu12/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nccl-cu12/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nvjitlink-cu12/nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nvtx-cu12/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 29))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 30))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 31))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 32))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 33))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/shtab/shtab-1.7.1-py3-none-any.whl (from -r requirements.txt (line 34))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 35))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 36))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 37))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 38))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torch/torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (from -r requirements.txt (line 39))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.43.1-py3-none-any.whl (from -r requirements.txt (line 40))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/triton/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 41))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/trl/trl-0.8.1-py3-none-any.whl (from -r requirements.txt (line 42))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/typing-extensions/typing_extensions-4.8.0-py3-none-any.whl (from -r requirements.txt (line 43))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tyro/tyro-0.7.3-py3-none-any.whl (from -r requirements.txt (line 44))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 45))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.2.7-py2.py3-none-any.whl (from -r requirements.txt (line 46))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.14.1->-r requirements.txt (line 5)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 7)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.24.2->-r requirements.txt (line 8)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.43.1->-r requirements.txt (line 40)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro==0.7.3->-r requirements.txt (line 44)) (13.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (2.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0->-r requirements.txt (line 39)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0->-r requirements.txt (line 39)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (0.1.0)\u001b[0m\n",
      "\u001b[34mscipy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=6777cee68784c31ae9fd3c38cc9539863dec1f834f4c5b0cd9bf12610e58839b\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[34mSuccessfully built fire\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, Brotli, bitsandbytes, typing-extensions, triton, tokenize-rt, termcolor, shtab, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, safetensors, pyzstd, pyppmd, pycryptodomex, pybcj, pathspec, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, multivolumefile, loralib, inflate64, docstring-parser, py7zr, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, fire, black, tyro, tokenizers, nvidia-cusolver-cu12, transformers, torch, datasets, accelerate, trl, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing_extensions 4.7.1\u001b[0m\n",
      "\u001b[34mUninstalling typing_extensions-4.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing_extensions-4.7.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: triton\u001b[0m\n",
      "\u001b[34mFound existing installation: triton 2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mUninstalling triton-2.0.0.dev20221202:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled triton-2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.20.3\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.20.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.20.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.16.1\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.16.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.16.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 accelerate-0.33.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 docstring-parser-0.16 fire-0.5.0 huggingface-hub-0.24.2 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pyzstd-0.15.9 safetensors-0.4.2 sagemaker-jumpstart-huggingface-script-utilities-1.2.7 sagemaker-jumpstart-script-utilities-1.1.9 shtab-1.7.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 tokenizers-0.19.1 torch-2.2.0 transformers-4.43.1 triton-2.2.0 trl-0.8.1 typing-extensions-4.8.0 tyro-0.7.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2025-04-13 14:27:58,722 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-04-13 14:27:58,722 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-04-13 14:27:58,782 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-04-13 14:27:58,829 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-04-13 14:27:58,876 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-04-13 14:27:58,885 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"batch_size\": \"4\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"chat_template\": \"Llama3.1\",\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"5\",\n",
      "        \"instruction_tuned\": \"True\",\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"target_modules\": \"q_proj,v_proj\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-3-2-1b-2025-04-13-14-20-50-328\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"batch_size\":\"4\",\"chat_dataset\":\"False\",\"chat_template\":\"Llama3.1\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"target_modules\":\"q_proj,v_proj\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"batch_size\":\"4\",\"chat_dataset\":\"False\",\"chat_template\":\"Llama3.1\",\"enable_fsdp\":\"True\",\"epoch\":\"5\",\"instruction_tuned\":\"True\",\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"target_modules\":\"q_proj,v_proj\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-3-2-1b-2025-04-13-14-20-50-328\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--batch_size\",\"4\",\"--chat_dataset\",\"False\",\"--chat_template\",\"Llama3.1\",\"--enable_fsdp\",\"True\",\"--epoch\",\"5\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--target_modules\",\"q_proj,v_proj\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_DATASET=False\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_TEMPLATE=Llama3.1\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=5\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=True\u001b[0m\n",
      "\u001b[34mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TARGET_MODULES=q_proj,v_proj\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --batch_size 4 --chat_dataset False --chat_template Llama3.1 --enable_fsdp True --epoch 5 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --target_modules q_proj,v_proj --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34m2025-04-13 14:27:58,913 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '4', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '5', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--target_modules', 'q_proj,v_proj', '--chat_template', 'Llama3.1', '--enable_fsdp', '--add_input_output_demarcation_key', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[34m[2025-04-13 14:28:03,110] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2025-04-13 14:28:03,110] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2025-04-13 14:28:03,110] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2025-04-13 14:28:03,110] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 2. Rank is 2\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 2\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 3. Rank is 3\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 3\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 1. Rank is 1\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 1\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 15709.00it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1871.62it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 64 examples [00:00, 27271.71 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/64 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 64/64 [00:00<00:00, 9084.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/64 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 64/64 [00:00<00:00, 2833.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/64 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 64/64 [00:00<00:00, 2711.06 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34m--> /opt/ml/additonals3data has 1235.8144 Million params\u001b[0m\n",
      "\u001b[34mtrainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.06889230774326355\u001b[0m\n",
      "\u001b[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[34mtrainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.06889230774326355\u001b[0m\n",
      "\u001b[34mtrainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.06889230774326355\u001b[0m\n",
      "\u001b[34mtrainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.06889230774326355\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mINFO:root:--> Training Set Length = 14\u001b[0m\n",
      "\u001b[34mINFO:root:--> Validation Set Length = 4\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.3+cuda12.3\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 1/1 [00:02<00:00,  2.29s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 1/1 [00:02<00:00,  2.29s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 1/1 [00:02<00:00,  2.30s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 1/1 [00:02<00:00,  2.31s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 1/1 [00:02<00:00,  2.29s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 1/1 [00:02<00:00,  2.29s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.8529396057128906\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 1/1 [00:02<00:00,  2.42s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0: 100%|#033[34m██████████#033[0m| 1/1 [00:02<00:00,  2.42s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 7 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 8 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 7 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 3 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\u001b[0m\n",
      "\u001b[34mWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\u001b[0m\n",
      "\u001b[34mWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mWe detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.35it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.36it/s]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.35it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.39it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.36it/s]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.36it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.39it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.36it/s]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(6.5678, device='cuda:0') eval_epoch_loss=tensor(1.8822, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 0 is 1.8821735382080078\u001b[0m\n",
      "\u001b[34mEpoch 1: train_perplexity=6.4940, train_epoch_loss=1.8709, epcoh time 2.6937802350000197s\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.846664547920227\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.72s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch1: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.72s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 7 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 8 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 7 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 4 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.37it/s]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.37it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.37it/s]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.37it/s]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.37it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.37it/s]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(6.5160, device='cuda:0') eval_epoch_loss=tensor(1.8743, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 1 is 1.8742560148239136\u001b[0m\n",
      "\u001b[34mEpoch 2: train_perplexity=6.4574, train_epoch_loss=1.8652, epcoh time 2.002860030000022s\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.8377708196640015\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch2: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 7 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 8 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 7 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 4 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.37it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.37it/s]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(6.4688, device='cuda:0') eval_epoch_loss=tensor(1.8670, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 2 is 1.8669978380203247\u001b[0m\n",
      "\u001b[34mEpoch 3: train_perplexity=6.4035, train_epoch_loss=1.8568, epcoh time 1.9916082309999865s\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.8288923501968384\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch3: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 7 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 8 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 7 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 4 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.39it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.39it/s]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.39it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.39it/s]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(6.4235, device='cuda:0') eval_epoch_loss=tensor(1.8600, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 3 is 1.8599668741226196\u001b[0m\n",
      "\u001b[34mEpoch 4: train_perplexity=6.3489, train_epoch_loss=1.8483, epcoh time 1.9937783190000005s\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4:   0%|#033[34m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 1.8196521997451782\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.70s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch4: 100%|#033[34m██████████#033[0m| 1/1 [00:01<00:00,  1.71s/it]\u001b[0m\n",
      "\u001b[34mMax CUDA memory allocated was 7 GB\u001b[0m\n",
      "\u001b[34mMax CUDA memory reserved was 8 GB\u001b[0m\n",
      "\u001b[34mPeak active CUDA memory was 7 GB\u001b[0m\n",
      "\u001b[34mCuda Malloc retires : 0\u001b[0m\n",
      "\u001b[34mCPU Total Peak Memory consumed during the train (max): 4 GB\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]#015evaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch:   0%|#033[32m          #033[0m| 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]\u001b[0m\n",
      "\u001b[34mevaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]\u001b[0m\n",
      "\u001b[34m#015evaluating Epoch: 100%|#033[32m██████████#033[0m| 1/1 [00:00<00:00,  1.38it/s]\u001b[0m\n",
      "\u001b[34meval_ppl=tensor(6.3813, device='cuda:0') eval_epoch_loss=tensor(1.8534, device='cuda:0')\u001b[0m\n",
      "\u001b[34mwe are about to save the PEFT modules\u001b[0m\n",
      "\u001b[34mPEFT modules are saved in saved_peft_model directory\u001b[0m\n",
      "\u001b[34mbest eval loss on epoch 4 is 1.853371500968933\u001b[0m\n",
      "\u001b[34mEpoch 5: train_perplexity=6.2985, train_epoch_loss=1.8403, epcoh time 1.9934626680000065s\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_prep, Value: 6.400459289550781\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_train_loss, Value: 1.8563085794448853\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_prep, Value: 6.471480846405029\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_eval_loss, Value: 1.867353081703186\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_epoch_time, Value: 2.135097896600007\u001b[0m\n",
      "\u001b[34mINFO:root:Key: avg_checkpoint_time, Value: 0.6260730737999893\u001b[0m\n",
      "\n",
      "2025-04-13 14:28:50 Uploading - Uploading generated training model\u001b[34mINFO:root:Combining pre-trained base model with the PEFT adapter module.\u001b[0m\n",
      "\u001b[34mINFO:root:Saving the combined model in safetensors format.\u001b[0m\n",
      "\u001b[34mINFO:root:Saving complete.\u001b[0m\n",
      "\u001b[34mINFO:root:Copying tokenizer to the output directory.\u001b[0m\n",
      "\u001b[34mINFO:root:Putting inference code with the fine-tuned model directory.\u001b[0m\n",
      "\u001b[34m2025-04-13 14:28:47,007 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-04-13 14:28:47,008 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-04-13 14:28:47,008 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2025-04-13 14:29:13 Completed - Training job completed\n",
      "Training seconds: 426\n",
      "Billable seconds: 426\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    environment={\"accept_eula\": \"true\"},  # EULA 동의\n",
    "    disable_output_compression=True,\n",
    "    instance_type=\"ml.g5.12xlarge\",       # 권장 인스턴스: ml.g5 or ml.p3[7]\n",
    ")\n",
    "\n",
    "estimator.set_hyperparameters(instruction_tuned=\"True\", epoch=\"5\", max_input_length=\"1024\", batch_size=\"4\")\n",
    "estimator.fit({\"training\": train_data_location})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf19db84-a902-4a3d-951d-5261c07bc5db",
   "metadata": {},
   "source": [
    "# Deployment \n",
    "파인튜닝한 모델의 결과물은, sagemaker container 내부 `/opt/ml/model`에 저장되는데 훈련이 종료되는 시점에 자동으로 S3에 업로드 됩니다. 업로드 S3 위치는 (특별히 지정하지 않는다면) Sagemaker의 기본 버킷하위에 `{job_name}/output/model/`위치에 저장됩니다. \n",
    "- 해당 bucket의 위치를 model 파일로 하여 deploy 할 수 도 있고,\n",
    "- 아래처럼 훈련 제어 변수인 `estimator.deploy()`를 통해서 배포할 수 도 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dd0a3e62-0c27-45ca-b205-2c5e244626fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for inference hosting endpoint. Defaulting to ml.g6.xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g6.xlarge.\n",
      "INFO:sagemaker:Creating model with name: meta-textgeneration-llama-3-2-1b-2025-04-13-14-46-44-854\n",
      "INFO:sagemaker:Creating endpoint-config with name 04-llama-basic-custom-dataset-02\n",
      "INFO:sagemaker:Creating endpoint with name 04-llama-basic-custom-dataset-02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = \"04-llama-basic-custom-dataset-02\"\n",
    "finetuned_predictor = estimator.deploy(endpoint_name=endpoint_name) # 엔트포인트 이름에 언더바`_` 불가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9812d875-ddc4-42a2-8453-6f11ffb6e0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의 이상재는 AWS의'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name = \"04-llama-basic-custom-dataset-02\"\n",
    "template_inference = template['prompt']\n",
    "\n",
    "instruction = \"최근 학습한 내용을 기반으로 다음 질문에 답하시오.\"\n",
    "# context = \"2025년 KCC와 AWS가 계획한 행사는 무엇인가요?\"\n",
    "context = \"AWS의 이상재는 누구인가?\"\n",
    "input_output_marker = \"\\n\\n### Response:\\n\"\n",
    "prompt = template[\"prompt\"].format(instruction=instruction, context=context)\n",
    "prompt += input_output_marker\n",
    "\n",
    "prompt_payload = {\n",
    "    'inputs':prompt,\n",
    "    'parameters': {'max_new_tokens': 512, 'top_p': 0.9,'temperature': 0.0}\n",
    "    # 'parameters': {'max_new_tokens': 512, 'top_p': 0.9,'temperature': 0.6, 'details': True}\n",
    "}\n",
    "response = finetuned_predictor.predict(prompt_payload)\n",
    "response['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "18fe50d7-cace-468e-bb74-44ec81c0806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template_inference = template['prompt']\n",
    "\n",
    "instruction = \"다음 질문에 답하시오.\"\n",
    "context = \"2025년 4월 sLM에 대하여 세션 발표를 진행한 AWS 사람은 누구인가요?\"\n",
    "# context = \"이상재는 누구인가?\"\n",
    "# context = \"AWS와 KCC 협력의 목적은 무엇인가요?\"\n",
    "# context = \"2025년 4월 15일에 AWS와 KCC 협력에 발표하는 SA는?\"\n",
    "# context = \"AWS의 이상재는 누구인가?\"\n",
    "\n",
    "input_output_marker = \"\\n\\n### Response:\\n\"\n",
    "prompt = template[\"prompt\"].format(instruction=instruction, context=context)\n",
    "prompt += input_output_marker\n",
    "\n",
    "generation_config = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"eos_token_id\": 128009,          # Llama 3.2 공식 EOS 토큰[1][3]\n",
    "    \"pad_token_id\": 128009,          # 패딩 토큰 통일[4]\n",
    "    \"repetition_penalty\": 1.2,       # 반복 문구 억제[3]\n",
    "    \"temperature\": 0.0,    \n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    # 'details': True,\n",
    "}\n",
    "\n",
    "prompt_payload = {\n",
    "    'inputs':prompt,\n",
    "    'parameters': generation_config\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5205c507-934a-430a-b75c-13f40036af39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AWS는 Amazon Web Services의 약자로, 미국에서 설립된 기업이다.\\n'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = finetuned_predictor.predict(prompt_payload)\n",
    "response['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c4649a-5cdf-4894-9395-157e3c560aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b10ac-ec1c-41ed-9850-28db466e9b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb0ee24-f2d1-4cc8-84e1-8822276e08a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
